{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e425ef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from hpsklearn import HyperoptEstimator\n",
    "from hpsklearn import any_classifier\n",
    "from hpsklearn import any_preprocessing\n",
    "from hyperopt import tpe\n",
    "import glob\n",
    "from hyperopt import STATUS_OK, Trials\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from hpsklearn import HyperoptEstimator, any_sparse_classifier, tfidf\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8efbf694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324,)\n"
     ]
    }
   ],
   "source": [
    "y=[]\n",
    "for i in range(18):\n",
    "    y= np.append(y,[i]*18)\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4834b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "\n",
    "path0 = \"/home/nest/Downloads/poses_w1_d8/p0\"\n",
    "all_files0 = glob.glob(path0 + \"/*.csv\")\n",
    "for filename0 in all_files0:\n",
    "    df = pd.read_csv(filename0, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path1 = \"/home/nest/Downloads/poses_w1_d8/p1\"\n",
    "all_files1 = glob.glob(path1 + \"/*.csv\")\n",
    "for filename1 in all_files1:\n",
    "    df = pd.read_csv(filename1, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path2 = \"/home/nest/Downloads/poses_w1_d8/p2\"\n",
    "all_files2 = glob.glob(path2 + \"/*.csv\")\n",
    "for filename2 in all_files2:\n",
    "    df = pd.read_csv(filename2, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path3 = \"/home/nest/Downloads/poses_w1_d8/p3\"\n",
    "all_files3 = glob.glob(path3 + \"/*.csv\")\n",
    "for filename3 in all_files3:\n",
    "    df = pd.read_csv(filename3, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path4 = \"/home/nest/Downloads/poses_w1_d8/p4\"\n",
    "all_files4 = glob.glob(path4 + \"/*.csv\")\n",
    "for filename4 in all_files4:\n",
    "    df = pd.read_csv(filename4, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path5 = \"/home/nest/Downloads/poses_w1_d8/p5\"\n",
    "all_files5 = glob.glob(path5 + \"/*.csv\")\n",
    "for filename5 in all_files5:\n",
    "    df = pd.read_csv(filename5, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path6 = \"/home/nest/Downloads/poses_w1_d8/p6\"\n",
    "all_files6 = glob.glob(path6 + \"/*.csv\")\n",
    "for filename6 in all_files6:\n",
    "    df = pd.read_csv(filename6, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path7 = \"/home/nest/Downloads/poses_w1_d8/p7\"\n",
    "all_files7 = glob.glob(path7 + \"/*.csv\")\n",
    "for filename7 in all_files7:\n",
    "    df = pd.read_csv(filename7, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path8 = \"/home/nest/Downloads/poses_w1_d8/p8\"\n",
    "all_files8 = glob.glob(path8 + \"/*.csv\")\n",
    "for filename8 in all_files8:\n",
    "    df = pd.read_csv(filename8, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "\n",
    "path9 = \"/home/nest/Downloads/poses_w1_d8/p9\"\n",
    "all_files9 = glob.glob(path9 + \"/*.csv\")\n",
    "for filename9 in all_files9:\n",
    "    df = pd.read_csv(filename9, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path10 = \"/home/nest/Downloads/poses_w1_d8/p10\"\n",
    "all_files10 = glob.glob(path10 + \"/*.csv\")\n",
    "for filename10 in all_files10:\n",
    "    df = pd.read_csv(filename10, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "path11 = \"/home/nest/Downloads/poses_w1_d8/p11\"\n",
    "all_files11 = glob.glob(path11 + \"/*.csv\")\n",
    "for filename11 in all_files11:\n",
    "    df = pd.read_csv(filename11, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path12 = \"/home/nest/Downloads/poses_w1_d8/p12\"\n",
    "all_files12 = glob.glob(path12 + \"/*.csv\")\n",
    "for filename12 in all_files12:\n",
    "    df = pd.read_csv(filename12, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path13 = \"/home/nest/Downloads/poses_w1_d8/p13\"\n",
    "all_files13 = glob.glob(path13 + \"/*.csv\")\n",
    "for filename13 in all_files13:\n",
    "    df = pd.read_csv(filename13, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "path14 = \"/home/nest/Downloads/poses_w1_d8/p14\"\n",
    "all_files14 = glob.glob(path14 + \"/*.csv\")\n",
    "for filename14 in all_files14:\n",
    "    df = pd.read_csv(filename14, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path15 = \"/home/nest/Downloads/poses_w1_d8/p15\"\n",
    "all_files15 = glob.glob(path15 + \"/*.csv\")\n",
    "for filename15 in all_files15:\n",
    "    df = pd.read_csv(filename15, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path16 = \"/home/nest/Downloads/poses_w1_d8/p16\"\n",
    "all_files16 = glob.glob(path16 + \"/*.csv\")\n",
    "for filename16 in all_files16:\n",
    "    df = pd.read_csv(filename16, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "\n",
    "path17 = \"/home/nest/Downloads/poses_w1_d8/p17\"\n",
    "all_files17 = glob.glob(path17 + \"/*.csv\")\n",
    "for filename17 in all_files17:\n",
    "    df = pd.read_csv(filename17, index_col=0, header=0)\n",
    "    X.append(df)\n",
    "nsamples1, nx1, ny1 = np.shape(X)\n",
    "X_2d = np.reshape(X,(nsamples1, nx1 * ny1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa932a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2d, y, stratify=y,\n",
    "                                                random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc07180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y)\n",
    "\n",
    "nsamples, nx, ny = np.shape(X_train)\n",
    "X_train_2d = np.reshape(X_train,(nsamples, nx * ny))\n",
    "\n",
    "#print(np.shape(X_test))\n",
    "nsamples1, nx1, ny1 = np.shape(X_test)\n",
    "X_test_2d = np.reshape(X_test,(nsamples1, nx1 * ny1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68d04926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HyperoptEstimator(\n",
    "    classifier=any_classifier('cla')\n",
    "    , preprocessing=any_preprocessing('pre')\n",
    "    , algo=tpe.suggest\n",
    "    , max_evals=200\n",
    "#     , trial_timeout=30\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebd92b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████| 1/1 [00:01<00:00,  1.93s/trial, best loss: 0.5918367346938775]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.41trial/s, best loss: 0.44897959183673475]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11.28trial/s, best loss: 0.44897959183673475]\n",
      "100%|██████████| 4/4 [00:03<00:00,  3.89s/trial, best loss: 0.44897959183673475]\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.11trial/s, best loss: 0.44897959183673475]\n",
      "100%|██████████| 6/6 [00:00<00:00,  4.44trial/s, best loss: 0.44897959183673475]\n",
      "100%|██████████| 7/7 [00:00<00:00, 14.78trial/s, best loss: 0.44897959183673475]\n",
      "100%|██████████| 8/8 [00:00<00:00,  2.40trial/s, best loss: 0.44897959183673475]\n",
      "100%|██████████| 9/9 [00:00<00:00, 19.85trial/s, best loss: 0.36734693877551017]\n",
      "100%|████████| 10/10 [00:00<00:00,  2.25trial/s, best loss: 0.36734693877551017]\n",
      "100%|████████| 11/11 [00:00<00:00,  7.37trial/s, best loss: 0.36734693877551017]\n",
      "100%|████████| 12/12 [00:00<00:00,  1.16trial/s, best loss: 0.36734693877551017]\n",
      "100%|████████| 13/13 [00:00<00:00,  1.03trial/s, best loss: 0.36734693877551017]\n",
      "100%|████████| 14/14 [00:00<00:00, 11.56trial/s, best loss: 0.36734693877551017]\n",
      "100%|████████| 15/15 [00:00<00:00,  1.76trial/s, best loss: 0.36734693877551017]\n",
      "100%|████████| 16/16 [00:00<00:00,  1.21trial/s, best loss: 0.36734693877551017]\n",
      " 94%|████████████████████████████████▉  | 16/17 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:41:58] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:41:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|████████| 17/17 [00:01<00:00,  1.92s/trial, best loss: 0.36734693877551017]\n",
      "100%|████████| 18/18 [00:03<00:00,  3.03s/trial, best loss: 0.36734693877551017]\n",
      "100%|████████| 19/19 [00:00<00:00,  7.45trial/s, best loss: 0.36734693877551017]\n",
      "100%|████████| 20/20 [00:00<00:00, 20.51trial/s, best loss: 0.36734693877551017]\n",
      "100%|████████| 21/21 [00:01<00:00,  1.27s/trial, best loss: 0.34693877551020413]\n",
      "100%|████████| 22/22 [00:02<00:00,  2.03s/trial, best loss: 0.34693877551020413]\n",
      "100%|████████| 23/23 [00:02<00:00,  2.27s/trial, best loss: 0.34693877551020413]\n",
      "100%|████████| 24/24 [00:00<00:00,  1.86trial/s, best loss: 0.34693877551020413]\n",
      "100%|████████| 25/25 [00:02<00:00,  2.29s/trial, best loss: 0.34693877551020413]\n",
      " 96%|█████████████████████████████████▋ | 25/26 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:42:12] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:42:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|████████| 26/26 [00:15<00:00, 15.23s/trial, best loss: 0.34693877551020413]\n",
      "100%|████████| 27/27 [00:00<00:00,  2.56trial/s, best loss: 0.34693877551020413]\n",
      "100%|████████| 28/28 [00:00<00:00, 10.72trial/s, best loss: 0.34693877551020413]\n",
      "100%|████████| 29/29 [00:00<00:00, 10.95trial/s, best loss: 0.34693877551020413]\n",
      " 97%|█████████████████████████████████▊ | 29/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:42:28] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:42:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|████████| 30/30 [00:17<00:00, 17.17s/trial, best loss: 0.34693877551020413]\n",
      "100%|████████| 31/31 [00:00<00:00,  8.01trial/s, best loss: 0.34693877551020413]\n",
      "100%|████████| 32/32 [00:00<00:00, 11.27trial/s, best loss: 0.34693877551020413]\n",
      "100%|████████| 33/33 [00:01<00:00,  1.64s/trial, best loss: 0.34693877551020413]\n",
      "100%|██████████| 34/34 [00:02<00:00,  2.94s/trial, best loss: 0.326530612244898]\n",
      "100%|██████████| 35/35 [00:00<00:00, 10.42trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 36/36 [00:00<00:00,  6.73trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 37/37 [00:00<00:00,  4.98trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 38/38 [00:01<00:00,  1.11s/trial, best loss: 0.326530612244898]\n",
      "100%|██████████| 39/39 [00:07<00:00,  7.85s/trial, best loss: 0.326530612244898]\n",
      "100%|██████████| 40/40 [00:00<00:00,  7.80trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 41/41 [00:00<00:00,  7.35trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 42/42 [00:00<00:00,  8.49trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 43/43 [00:00<00:00,  4.97trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 44/44 [00:01<00:00,  1.14s/trial, best loss: 0.326530612244898]\n",
      "100%|██████████| 45/45 [00:00<00:00, 10.70trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 46/46 [00:00<00:00,  2.68trial/s, best loss: 0.326530612244898]\n",
      " 98%|██████████████████████████████████▎| 46/47 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:43:03] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:43:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 47/47 [00:00<00:00,  2.11trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 48/48 [00:00<00:00,  4.90trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 49/49 [00:00<00:00,  6.85trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 50/50 [00:00<00:00,  6.44trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 51/51 [00:00<00:00,  4.10trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 52/52 [00:00<00:00,  6.95trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 53/53 [00:00<00:00,  5.34trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 54/54 [00:00<00:00,  7.46trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 55/55 [00:00<00:00,  7.34trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 56/56 [00:00<00:00,  7.00trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 57/57 [00:00<00:00,  3.81trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 58/58 [00:00<00:00,  3.43trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 59/59 [00:00<00:00,  9.13trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 60/60 [00:00<00:00,  3.50trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 61/61 [00:02<00:00,  2.78s/trial, best loss: 0.326530612244898]\n",
      "100%|██████████| 62/62 [00:01<00:00,  1.83s/trial, best loss: 0.326530612244898]\n",
      "100%|██████████| 63/63 [00:00<00:00,  5.50trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 64/64 [00:08<00:00,  8.97s/trial, best loss: 0.326530612244898]\n",
      "100%|██████████| 65/65 [00:00<00:00,  3.84trial/s, best loss: 0.326530612244898]\n",
      " 98%|██████████████████████████████████▍| 65/66 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:43:21] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 66/66 [00:10<00:00, 10.83s/trial, best loss: 0.326530612244898]\n",
      "100%|██████████| 67/67 [00:00<00:00,  8.75trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 68/68 [00:00<00:00,  7.43trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 69/69 [00:01<00:00,  1.78s/trial, best loss: 0.326530612244898]\n",
      "100%|██████████| 70/70 [00:00<00:00, 10.87trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 71/71 [00:00<00:00,  6.26trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 72/72 [00:00<00:00,  5.91trial/s, best loss: 0.326530612244898]\n",
      "100%|██████████| 73/73 [00:00<00:00,  5.36trial/s, best loss: 0.326530612244898]\n",
      "100%|████████| 74/74 [00:00<00:00,  1.87trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 75/75 [00:00<00:00,  4.48trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 76/76 [00:00<00:00,  2.40trial/s, best loss: 0.30612244897959184]\n",
      " 99%|██████████████████████████████████▌| 76/77 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:43:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:43:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|████████| 77/77 [00:05<00:00,  5.38s/trial, best loss: 0.30612244897959184]\n",
      "100%|████████| 78/78 [00:10<00:00, 10.73s/trial, best loss: 0.30612244897959184]\n",
      "100%|████████| 79/79 [00:00<00:00,  1.46trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 80/80 [00:00<00:00,  2.49trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 81/81 [00:00<00:00,  6.35trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 82/82 [00:00<00:00,  5.65trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 83/83 [00:00<00:00,  7.54trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 84/84 [00:00<00:00,  8.67trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 85/85 [00:00<00:00,  6.62trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 86/86 [00:00<00:00,  6.24trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 87/87 [00:00<00:00,  7.93trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 88/88 [00:00<00:00,  6.26trial/s, best loss: 0.30612244897959184]\n",
      " 99%|██████████████████████████████████▌| 88/89 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:43:55] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:43:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|████████| 89/89 [00:11<00:00, 11.43s/trial, best loss: 0.30612244897959184]\n",
      "100%|████████| 90/90 [00:00<00:00,  3.55trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 91/91 [00:00<00:00,  3.88trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 92/92 [00:00<00:00,  1.11trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 93/93 [00:00<00:00, 10.74trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 94/94 [00:00<00:00,  2.12trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 95/95 [00:00<00:00,  6.38trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 96/96 [00:00<00:00,  9.42trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 97/97 [00:02<00:00,  2.17s/trial, best loss: 0.30612244897959184]\n",
      "100%|████████| 98/98 [00:00<00:00,  6.63trial/s, best loss: 0.30612244897959184]\n",
      "100%|████████| 99/99 [00:07<00:00,  7.54s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 100/100 [00:00<00:00,  9.15trial/s, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▋| 100/101 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████| 101/101 [00:00<00:00,  5.24trial/s, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▋| 101/102 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:44:20] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:44:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████| 102/102 [00:03<00:00,  3.34s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 103/103 [00:00<00:00,  1.39trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 104/104 [00:01<00:00,  1.04s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 105/105 [00:00<00:00,  8.88trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 106/106 [00:00<00:00,  9.03trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 107/107 [00:00<00:00,  7.07trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 108/108 [00:00<00:00,  6.35trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 109/109 [00:00<00:00,  6.86trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 110/110 [00:03<00:00,  3.56s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 111/111 [00:00<00:00,  4.06trial/s, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▋| 111/112 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:44:30] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:44:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████| 112/112 [00:16<00:00, 16.87s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 113/113 [00:00<00:00,  4.83trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 114/114 [00:30<00:00, 30.11s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 115/115 [00:00<00:00,  4.99trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 116/116 [00:00<00:00,  5.50trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 117/117 [00:00<00:00,  9.38trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 118/118 [00:00<00:00,  2.33trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 119/119 [00:00<00:00,  1.55trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 120/120 [00:00<00:00,  9.24trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 121/121 [00:00<00:00,  2.51trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 122/122 [00:01<00:00,  1.42s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 123/123 [00:00<00:00, 10.07trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 124/124 [00:00<00:00,  1.76trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 125/125 [00:00<00:00,  5.30trial/s, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▋| 125/126 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:45:22] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:45:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████| 126/126 [00:03<00:00,  3.59s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 127/127 [00:00<00:00,  1.94trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 128/128 [00:00<00:00,  5.42trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 129/129 [00:00<00:00,  1.75trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 130/130 [00:00<00:00,  1.17trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 131/131 [00:00<00:00,  3.04trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 132/132 [00:00<00:00,  8.32trial/s, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▊| 132/133 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=12054594).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████| 133/133 [00:02<00:00,  2.23s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 134/134 [00:00<00:00,  5.82trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 135/135 [00:00<00:00,  7.57trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 136/136 [00:01<00:00,  1.11s/trial, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▊| 136/137 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:45:33] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:45:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████| 137/137 [00:12<00:00, 12.47s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 138/138 [00:00<00:00,  3.10trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 139/139 [00:01<00:00,  1.90s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 140/140 [00:00<00:00,  5.90trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 141/141 [00:00<00:00,  3.67trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 142/142 [00:00<00:00,  4.80trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 143/143 [00:00<00:00,  2.15trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 144/144 [00:00<00:00,  2.38trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 145/145 [00:00<00:00,  5.32trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 146/146 [00:00<00:00,  4.50trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 147/147 [00:00<00:00,  3.69trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 148/148 [00:00<00:00,  1.69trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 149/149 [00:00<00:00,  1.25trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 150/150 [00:00<00:00,  1.25trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 151/151 [00:00<00:00,  3.19trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 152/152 [00:01<00:00,  1.67s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 153/153 [00:00<00:00,  1.23trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 154/154 [00:00<00:00,  2.45trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 155/155 [00:00<00:00,  1.77trial/s, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▊| 155/156 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:45:56] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:45:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████| 156/156 [00:00<00:00,  1.29trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 157/157 [00:03<00:00,  3.00s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 158/158 [00:00<00:00,  5.72trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 159/159 [00:02<00:00,  2.06s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 160/160 [00:04<00:00,  4.70s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 161/161 [00:00<00:00,  8.17trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 162/162 [00:00<00:00,  3.11trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 163/163 [00:00<00:00,  4.31trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 164/164 [00:00<00:00,  2.05trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 165/165 [00:00<00:00,  5.05trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 166/166 [00:00<00:00,  3.18trial/s, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▊| 166/167 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:46:09] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:46:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████| 167/167 [00:08<00:00,  8.36s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 168/168 [00:01<00:00,  1.09s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 169/169 [00:00<00:00,  1.49trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 170/170 [00:00<00:00,  1.90trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 171/171 [00:00<00:00,  7.15trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 172/172 [00:00<00:00,  8.48trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 173/173 [00:00<00:00,  5.66trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 174/174 [00:00<00:00,  3.18trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 175/175 [00:00<00:00,  9.79trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 176/176 [00:00<00:00,  3.10trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 177/177 [00:00<00:00,  7.91trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 178/178 [00:00<00:00,  5.16trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 179/179 [00:00<00:00,  5.15trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 180/180 [00:00<00:00,  6.94trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 181/181 [00:01<00:00,  1.37s/trial, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▊| 181/182 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:46:24] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:46:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████| 182/182 [00:04<00:00,  4.75s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 183/183 [00:00<00:00,  1.09trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 184/184 [00:00<00:00,  4.33trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 185/185 [00:00<00:00,  2.35trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 186/186 [00:00<00:00,  5.10trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 187/187 [00:01<00:00,  1.75s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 188/188 [00:00<00:00,  2.25trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 189/189 [00:00<00:00,  8.80trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 190/190 [00:00<00:00,  2.85trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 191/191 [00:00<00:00,  1.20trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 192/192 [00:00<00:00,  9.28trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 193/193 [00:00<00:00,  4.56trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 194/194 [00:00<00:00,  6.65trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 195/195 [00:00<00:00,  5.46trial/s, best loss: 0.30612244897959184]\n",
      " 99%|████████████████████████████████▊| 195/196 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nest/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:46:36] WARNING: ../src/learner.cc:576:                                      \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[11:46:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████| 196/196 [00:17<00:00, 17.46s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 197/197 [00:02<00:00,  2.67s/trial, best loss: 0.30612244897959184]\n",
      "100%|██████| 198/198 [00:00<00:00,  3.31trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 199/199 [00:00<00:00,  3.47trial/s, best loss: 0.30612244897959184]\n",
      "100%|██████| 200/200 [00:00<00:00,  1.44trial/s, best loss: 0.30612244897959184]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_2d, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f4a58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.593\n"
     ]
    }
   ],
   "source": [
    "# summarize performance\n",
    "acc = model.score(X_test_2d, y_test)\n",
    "print(\"Accuracy: %.3f\" % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8e3fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.593\n",
      "{'learner': RandomForestClassifier(max_features=None, n_estimators=338, n_jobs=1,\n",
      "                       random_state=3, verbose=False), 'preprocs': (StandardScaler(),), 'ex_preprocs': ()}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.3f\" % acc)\n",
    "print(model.best_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da9c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
